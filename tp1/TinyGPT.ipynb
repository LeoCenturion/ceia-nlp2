{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34275d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# TinyGPT\\n\\n**Author: Abraham R.**\\n\\nThe following notebook is an example of a really tiny GPT based model called TinyGPT.\\nYou'll review the GPT architecture (transformer decoder) and implement the following tasks:\\n\\n## TinyGPT Architecture\\n\\nTailored for the [NLP-II course](https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/CEIA-LLMIAG) as we deal with architectures and theory, this model consists of a **Mixture of Experts GPT**, equivalent to models like:\\n- DeepSeek\\n- Mistral\\n\\n## Tasks\\n\\nUsing TinyGPT you need to implement the following modifications:\\n\\n\\n## Inference: Modify the generate function to:\\n- Greedy decoding (pick max probability token).\\n- Temperature sampling.\\n- top-k or top-p sampling.\\n\\n### References\\n- [huggingface generate](https://huggingface.co/docs/transformers/main_classes/text_generation)\\n\\n## Architecture:\\n- Make TinyGPT a Mixture of Experts (MoE) of at least 2 experts.\\n\\n## What to expect?\\n\\n- You'll manage to understand a depth implementation of a GPT model.\\n- Implement a MoE Layer to create a state-of-the art GPT model.\\n- Explore decoding algorithms for text generation.\\n\\n\\n### NOTE\\n\\nTokenization is out of scope, we'll use a simple yet ineffective character-based tokenizer.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# TinyGPT\n",
    "\n",
    "**Author: Abraham R.**\n",
    "\n",
    "The following notebook is an example of a really tiny GPT based model called TinyGPT.\n",
    "You'll review the GPT architecture (transformer decoder) and implement the following tasks:\n",
    "\n",
    "## TinyGPT Architecture\n",
    "\n",
    "Tailored for the [NLP-II course](https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/CEIA-LLMIAG) as we deal with architectures and theory, this model consists of a **Mixture of Experts GPT**, equivalent to models like:\n",
    "- DeepSeek\n",
    "- Mistral\n",
    "\n",
    "## Tasks\n",
    "\n",
    "Using TinyGPT you need to implement the following modifications:\n",
    "\n",
    "\n",
    "## Inference: Modify the generate function to:\n",
    "- Greedy decoding (pick max probability token).\n",
    "- Temperature sampling.\n",
    "- top-k or top-p sampling.\n",
    "\n",
    "### References\n",
    "- [huggingface generate](https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
    "\n",
    "## Architecture:\n",
    "- Make TinyGPT a Mixture of Experts (MoE) of at least 2 experts.\n",
    "\n",
    "## What to expect?\n",
    "\n",
    "- You'll manage to understand a depth implementation of a GPT model.\n",
    "- Implement a MoE Layer to create a state-of-the art GPT model.\n",
    "- Explore decoding algorithms for text generation.\n",
    "\n",
    "\n",
    "### NOTE\n",
    "\n",
    "Tokenization is out of scope, we'll use a simple yet ineffective character-based tokenizer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc42cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Type\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e87f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06f210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ea4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Downloading Dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = httpx.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177a7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[:100_000]  # Using 100k characters for speedup\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "035eb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Character-based encoding\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5249c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return [stoi[c] for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "413a4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(l):\n",
    "    return \"\".join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "104bbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9534ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1522928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataloaders\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data: torch.Tensor, block_size: int):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.block_size]\n",
    "        y = self.data[idx + 1 : idx + self.block_size + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13400148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GPT Configuration\n",
    "@dataclass\n",
    "class MoEArgs:\n",
    "    \"\"\"\n",
    "    MoE input arguments class.\n",
    "    \"\"\"\n",
    "\n",
    "    num_experts: int = field(default=4)\n",
    "    num_experts_per_token: int = field(default=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9981ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    Base class for GPT models.\n",
    "    \"\"\"\n",
    "\n",
    "    block_size: int = 32\n",
    "    batch_size: int = 8\n",
    "    n_embd: int = 64\n",
    "    n_head: int = 4\n",
    "    n_layer: int = 2\n",
    "    dropout: float = 0.1\n",
    "    vocab_size: int = vocab_size\n",
    "    bias: bool = True\n",
    "    ff_class: Optional[Type[nn.Module]] = None\n",
    "    moe: Optional[MoEArgs] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57691b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_size': 32, 'batch_size': 512, 'n_embd': 64, 'n_head': 4, 'n_layer': 2, 'dropout': 0.1, 'vocab_size': 61, 'bias': True, 'ff_class': None, 'moe': None}\n"
     ]
    }
   ],
   "source": [
    "config = GPTConfig()\n",
    "config.batch_size = 512\n",
    "print(config.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffd55dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CharDataset(train_data, config.block_size)\n",
    "val_dataset = CharDataset(val_data, config.block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8693f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=8,  # if using mps set num_workers as 0.\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee03a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention Head for Multi-Head Attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args: GPTConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert args.n_embd % args.n_head == 0, \"n_embd must be divisible by n_head\"\n",
    "        self.head_dim = args.n_embd // args.n_head\n",
    "\n",
    "        # Combined QKV projection\n",
    "        self.key_query_value = nn.Linear(args.n_embd, 3 * self.head_dim, bias=args.bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.block_size = args.block_size\n",
    "        self.register_buffer(\n",
    "            \"tril\", torch.tril(torch.ones(args.block_size, args.block_size))\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        kv_cache: Optional[torch.Tensor] = None,\n",
    "        return_weights=False,\n",
    "    ):\n",
    "        B, T, C = x.shape\n",
    "        key_query_value = self.key_query_value(x)  # (B, T, 3 * head_dim)\n",
    "        k, q, v = torch.chunk(key_query_value, 3, dim=-1)  # (B, T, head_dim) each\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            key_cache, value_cache = kv_cache.unbind(dim=0)  # (B, T', head_dim)\n",
    "            k = torch.cat((key_cache, k), dim=1)\n",
    "            v = torch.cat((value_cache, v), dim=1)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_dim**-0.5)  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        out = wei @ v  # (B, T, head_dim)\n",
    "\n",
    "        if return_weights:\n",
    "            return out, wei\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            return out, torch.stack((k, v))\n",
    "\n",
    "        return out, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7de8ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, args: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert args.n_embd % args.n_head == 0, \"n_embd must be divisible by n_head\"\n",
    "        self.n_heads = args.n_head\n",
    "        self.head_dim = args.n_embd // args.n_head\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHead(args) for _ in range(self.n_heads)])\n",
    "\n",
    "        self.proj = nn.Linear(args.n_embd, args.n_embd, bias=args.bias)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "    def forward(self, x, kv_cache=None, return_weights=False):\n",
    "        all_outputs = []\n",
    "        all_weights = []\n",
    "        new_kv_cache = [] if kv_cache is not None else None\n",
    "\n",
    "        for i, head in enumerate(self.heads):\n",
    "            head_cache = kv_cache[i] if kv_cache is not None else None\n",
    "            out, weights_or_kv = head(\n",
    "                x, kv_cache=head_cache, return_weights=return_weights\n",
    "            )\n",
    "            all_outputs.append(out)\n",
    "            if return_weights:\n",
    "                all_weights.append(weights_or_kv)\n",
    "            if kv_cache is not None:\n",
    "                new_kv_cache.append(weights_or_kv)  # weights_or_kv is new kv_cache here\n",
    "\n",
    "        concat = torch.cat(all_outputs, dim=-1)  # concat along embedding dim\n",
    "        out = self.dropout(self.proj(concat))\n",
    "\n",
    "        if return_weights:\n",
    "            return out, torch.stack(all_weights)\n",
    "        if kv_cache is not None:\n",
    "            return out, new_kv_cache\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cfbfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d94a31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "\n",
    "        ff_class = config.ff_class if config.ff_class is not None else FeedForward\n",
    "        self.ff = ff_class(config)\n",
    "\n",
    "    def forward(self, x, kv_cache=None, return_weights=False):\n",
    "        attn_out = self.attn(\n",
    "            self.ln1(x), kv_cache=kv_cache, return_weights=return_weights\n",
    "        )\n",
    "        if return_weights:\n",
    "            attn_out, weights = attn_out\n",
    "        else:\n",
    "            weights = None\n",
    "\n",
    "        if isinstance(attn_out, tuple):\n",
    "            attn_out, updated_kv = attn_out\n",
    "        else:\n",
    "            updated_kv = None\n",
    "\n",
    "        x = x + attn_out\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return (x, updated_kv, weights) if return_weights else (x, updated_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5e09dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TinyGPT Architecture\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, idx, kv_cache=None, return_weights=False):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_emb(idx)\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        pos_emb = self.pos_emb(pos)[None, :, :]\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        new_kv_cache = [] if kv_cache is not None else None\n",
    "        all_weights = [] if return_weights else None\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            layer_kv = kv_cache[i] if kv_cache is not None else None\n",
    "            if return_weights:\n",
    "                x, updated_kv, weights = block(\n",
    "                    x, kv_cache=layer_kv, return_weights=True\n",
    "                )\n",
    "                all_weights.append(weights)  # weights shape: (n_heads, B, T, T)\n",
    "            else:\n",
    "                x, updated_kv = block(x, kv_cache=layer_kv)\n",
    "            if kv_cache is not None:\n",
    "                new_kv_cache.append(updated_kv)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        if return_weights:\n",
    "            if kv_cache is not None:\n",
    "                return logits, new_kv_cache, all_weights\n",
    "            else:\n",
    "                return logits, all_weights\n",
    "        else:\n",
    "            if kv_cache is not None:\n",
    "                return logits, new_kv_cache\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a611a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generation function (inference)\n",
    "@torch.no_grad()\n",
    "def generate(prompt: str, max_new_tokens: int = 100, use_cache: bool = True):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
    "    kv_cache = None\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        if use_cache and kv_cache is not None:\n",
    "            idx_cond = idx[:, -1:]\n",
    "        else:\n",
    "            idx_cond = idx[:, -config.block_size :]\n",
    "\n",
    "        out = model(idx_cond, kv_cache=kv_cache) if use_cache else model(idx_cond)\n",
    "\n",
    "        if isinstance(out, tuple):\n",
    "            logits, kv_cache = out\n",
    "        else:\n",
    "            logits = out\n",
    "            kv_cache = None\n",
    "\n",
    "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "    return decode(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44059e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "# # Setup\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f'Using device {device}')\n",
    "m = TinyGPT(config).to(device)\n",
    "model = torch.compile(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71a1b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7666703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 2.24524: 100%|█████████████████████████████████████████████████████████████████████████| 175/175 [00:34<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 2.2345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_loss 2.02072: 100%|███████████████████████████████████████████████████████████████████████| 19/19 [00:03<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation loss: 2.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 1.99682: 100%|█████████████████████████████████████████████████████████████████████████| 175/175 [00:28<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 training loss: 1.9872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_loss 1.74405: 100%|███████████████████████████████████████████████████████████████████████| 19/19 [00:01<00:00,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation loss: 1.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Training\n",
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_data_loader=train_loader,\n",
    "    test_data_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    save_dir=\"./checkpoints\",\n",
    "    save_every_n=500,\n",
    ")\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    avg_train_loss = trainer.train_model_v2(use_amp=True, dtype=torch.bfloat16)\n",
    "    print(f\"Epoch {epoch + 1} training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    val_loss = trainer.eval_model()\n",
    "    print(f\"Epoch {epoch + 1} validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "KdYmn-3KpZ56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c3fde7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To ber hath and come't provess mapoffueg his hims. BRose for ther sut, wave\\nHat sath and he wart with ves\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Quick test\n",
    "generate(\"To be\", max_new_tokens=100, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a711cc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "parameter without a default follows parameter with a default (3978634822.py, line 21)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmodel\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m parameter without a default follows parameter with a default\n"
     ]
    }
   ],
   "source": [
    "# # Task I\n",
    "#\n",
    "# Using TinyGPT you need to implement the following modifications:\n",
    "#\n",
    "#\n",
    "# ## Inference: Modify the generate function to:\n",
    "# - Greedy decoding (pick max probability token).\n",
    "# - Temperature sampling.\n",
    "# - top-k or top-p sampling.\n",
    "#\n",
    "# ### References\n",
    "# - [huggingface generate](https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
    "@torch.no_grad()\n",
    "def generateV2(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 100,\n",
    "    use_cache: bool = True,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 10,\n",
    "    top_p: float = 0.9,\n",
    "    model\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
    "    kv_cache = None\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        if use_cache and kv_cache is not None:\n",
    "            idx_cond = idx[:, -1:]\n",
    "        else:\n",
    "            idx_cond = idx[:, -config.block_size :]\n",
    "\n",
    "        out = model(idx_cond, kv_cache=kv_cache) if use_cache else model(idx_cond)\n",
    "\n",
    "        if isinstance(out, tuple):\n",
    "            logits, kv_cache = out\n",
    "        else:\n",
    "            logits = out\n",
    "            kv_cache = None\n",
    "\n",
    "        logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "        if temperature <= 0.0:  # Greedy decoding\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            # Temperature scaling\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Top-k filtering\n",
    "            if top_k > 0:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                kth_largest = v[:, [-1]]\n",
    "                logits[logits < kth_largest] = -float(\"Inf\")\n",
    "\n",
    "            # Top-p (nucleus) filtering\n",
    "            if top_p < 1.0:\n",
    "                probs_for_filter = F.softmax(logits, dim=-1)\n",
    "                sorted_probs, sorted_indices = torch.sort(\n",
    "                    probs_for_filter, descending=True\n",
    "                )\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "                # Remove tokens with cumulative probability above the threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                # Shift the indices to the right to keep also the first token above the threshold\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                # scatter sorted tensors to original indexing\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                    1, sorted_indices, sorted_indices_to_remove\n",
    "                )\n",
    "                logits[indices_to_remove] = -float(\"Inf\")\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "    return decode(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f4028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a07a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Compare generate vs generateV2\n",
    "#\n",
    "# Add your comments and findings\n",
    "result_1 = generate(\"To be\", max_new_tokens=100, use_cache=True)\n",
    "result_2 = generateV2(\"To be\", max_new_tokens=100, use_cache=True, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91067ac2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f'{result_1}')\n",
    "print(f'{result_2}')\n",
    "# exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a97a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Task II\n",
    "# - Make TinyGPT a Mixture of Experts (MoE) of at least 2 experts.\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    An expert MLP instance from within a MoE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig) -> None:\n",
    "        \"\"\"\n",
    "        Initiates expert MLP given dimensions/hidden dimensions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    \"\"\"\n",
    "    MoE gating network MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(config.n_embd, config.moe.num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f606f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of experts FeedForward Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, experts: List[nn.Module], gate: nn.Module, moe_args: MoEArgs):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.gate = gate\n",
    "        self.args = moe_args\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        expert_idx  = torch.argmax(torch.softmax(self.gate(x), dim=-1), dim=-1)\n",
    "        expert = self.experts[expert_idx]\n",
    "        return expert(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEFFN(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        moe_args = config.moe if config.moe is not None else MoEArgs()\n",
    "        self.moe = MoELayer(\n",
    "            experts=[Expert(config) for _ in range(moe_args.num_experts)],\n",
    "            gate=Gate(config),\n",
    "            moe_args=moe_args,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.moe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Adding MoE to Config\n",
    "config.ff_class = MoEFFN\n",
    "config.moe = MoEArgs(num_experts=4, num_experts_per_token=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training TinyGPT-MoE\n",
    "m_moe = TinyGPT(config).to(device)\n",
    "model_moe = torch.compile(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model_moe.parameters(), lr=1e-3)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model_moe,\n",
    "    train_data_loader=train_loader,\n",
    "    test_data_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    save_dir=\"./checkpoints\",\n",
    "    save_every_n=500,\n",
    ")\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    avg_train_loss = trainer.train_model_v2(use_amp=True, dtype=torch.bfloat16)\n",
    "    print(f\"Epoch {epoch + 1} training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    val_loss = trainer.eval_model()\n",
    "    print(f\"Epoch {epoch + 1} validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2xz2uaMvrrAW",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e13892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Visualizing Attention\n",
    "#\n",
    "# As we know, a GPT has the task to complete text, let's see the attention maps generated by a pretrained model\n",
    "@torch.no_grad()\n",
    "def visualize_attention(model, prompt, max_len=10):\n",
    "    model.eval()\n",
    "    idx = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
    "\n",
    "    # Run forward with return_weights=True\n",
    "    logits, all_weights = model(idx, return_weights=True)\n",
    "\n",
    "    # all_weights is a list of length n_layers\n",
    "    # each element: shape (n_heads, batch_size, seq_len, seq_len)\n",
    "    # We'll visualize the first batch element only\n",
    "\n",
    "    n_layers = len(all_weights)\n",
    "    n_heads = all_weights[0].shape[0]\n",
    "    seq_len = all_weights[0].shape[-1]\n",
    "\n",
    "    for layer_i in range(n_layers):\n",
    "        fig, axes = plt.subplots(1, n_heads, figsize=(5 * n_heads, 5))\n",
    "        if n_heads == 1:\n",
    "            axes = [axes]\n",
    "        for head_i in range(n_heads):\n",
    "            attn = all_weights[layer_i][head_i, 0].cpu()  # shape (seq_len, seq_len)\n",
    "            im = axes[head_i].imshow(attn, cmap=\"viridis\")\n",
    "            axes[head_i].set_title(f\"Layer {layer_i + 1} Head {head_i + 1}\")\n",
    "            axes[head_i].set_xlabel(\"Key Position\")\n",
    "            axes[head_i].set_ylabel(\"Query Position\")\n",
    "            axes[head_i].set_xticks(range(seq_len))\n",
    "            axes[head_i].set_yticks(range(seq_len))\n",
    "            fig.colorbar(im, ax=axes[head_i])\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"To be or not to be\"\n",
    "visualize_attention(model, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a620a",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "-\n",
    "-\n",
    "# Congratulations! 🎉\n",
    "\n",
    "After completing the tasks you've successfully pretrained for first GPT, remember to add your conclusions and findings! And you can now brag to your friend on how LLMs and GPTs work!\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
